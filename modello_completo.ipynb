{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINet COD10K Detection\n",
    "Questo notebook implementa SINet per il rilevamento di oggetti mimetizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from skimage.metrics import structural_similarity as ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametri ottimali: \n",
    "* Adam decay: 1e-4\n",
    "* Resize: 416 x 416\n",
    "* Batch: 40\n",
    "* Epochs: 180 (Provo con 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CODDataset(Dataset):\n",
    "    def __init__(self, image_folder, mask_folder,\n",
    "                 image_transform=None, mask_transform=None):\n",
    "        self.image_files = os.listdir(image_folder)\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_folder, self.image_files[idx].replace('.jpg', '.png'))\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomRotation(15),\n",
    "    # transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=(3, 3)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomRotation(15),\n",
    "    # transforms.GaussianBlur(kernel_size=(3, 3)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone encoder (ResNet Multi-scale)\n",
    "Estrae feature multi-scala con ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        resnet = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "        self.stage1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu) \n",
    "        self.pool = resnet.maxpool \n",
    "        self.stage2 = resnet.layer1  # 256 canali\n",
    "        self.stage3 = resnet.layer2  # 512 canali\n",
    "        self.stage4 = resnet.layer3  # 1024 canali\n",
    "        self.stage5 = resnet.layer4  # 2048 canali\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.stage1(x)  # 64 canali, 112x112\n",
    "        x1p = self.pool(x1)\n",
    "        x2 = self.stage2(x1p)  # 256 canali, 56x56\n",
    "        x3 = self.stage3(x2)  # 512 canali, 28x28\n",
    "        x4 = self.stage4(x3)  # 1024 canali, 14x14\n",
    "        x5 = self.stage5(x4)  # 2048 canali, 7x7\n",
    "        return x1, x2, x3, x4, x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDPM (Joint Domain perception module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JDPM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=256):\n",
    "        super(JDPM, self).__init__()\n",
    "        self.reduce_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(out_channels, out_channels, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reduce_channels(x)\n",
    "        return self.conv1(x) + self.conv3(x) + self.conv5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entanglement transformer block (ETB)\n",
    "Modella dipendenze tra frequenze e spazio con self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fft(x_freq):\n",
    "    return (x_freq - x_freq.mean()) / (x_freq.std() + 1e-8)  # Normalizzazione\n",
    "\n",
    "def filter_fft(x_freq, low_cut=5, high_cut=50):\n",
    "    # Creiamo una maschera per tenere solo certe frequenze\n",
    "    mask = (x_freq.abs() > low_cut) & (x_freq.abs() < high_cut)\n",
    "    return x_freq * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft as fft\n",
    "import torch.nn as nn\n",
    "\n",
    "class ETB(nn.Module):  \n",
    "    def __init__(self, in_channels=256):  \n",
    "        super(ETB, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "        # ðŸ”´ Convoluzione learnable per sostituire IFFT (per evitare numeri complessi)\n",
    "        self.reconstruction_layer = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def fourier_transform(self, x):\n",
    "        \"\"\" Applica la Trasformata di Fourier 2D usando solo il modulo per evitare problemi con MPS. \"\"\"\n",
    "        x_cpu = x.to(\"cpu\")  # Spostiamo su CPU temporaneamente\n",
    "        x_freq = fft.fft2(x_cpu, norm=\"ortho\")  \n",
    "        x_freq = fft.fftshift(x_freq)  \n",
    "\n",
    "        magnitude = torch.abs(x_freq)  # âœ… Usiamo solo il modulo\n",
    "        return magnitude.to(x.device)  # Ritorniamo a MPS senza numeri complessi\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # ðŸŸ¢ Applica la Fourier Transform (usiamo solo il modulo!)\n",
    "        magnitude = self.fourier_transform(x_norm)\n",
    "\n",
    "        # ðŸŸ¢ Sostituiamo IFFT con una convoluzione learnable\n",
    "        x_spatial = self.reconstruction_layer(magnitude)\n",
    "\n",
    "        return x_spatial + self.conv1(x_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual-Domain reverse parser (DRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRP(nn.Module):\n",
    "    def __init__(self, in_channels=256):\n",
    "        super(DRP, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def reverse_attention(self, x, mask):\n",
    "        return x * (1 - self.sigmoid(mask))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.conv(x)\n",
    "        return self.reverse_attention(x, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementazione della Focal Loss per segmentazione binaria.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  \n",
    "        self.gamma = gamma  \n",
    "        self.reduction = reduction  \n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = preds.view(-1)  \n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        bce_loss = F.binary_cross_entropy(preds, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)  \n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_loss(preds, targets, smooth=1.0):\n",
    "    \"\"\"\n",
    "    Implementazione della IoU Loss.\n",
    "    \"\"\"\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    intersection = (preds * targets).sum()\n",
    "    union = preds.sum() + targets.sum() - intersection\n",
    "    return 1 - ((intersection + smooth) / (union + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(preds, targets, smooth=1.0):\n",
    "    \"\"\"\n",
    "    Implementazione della Dice Loss.\n",
    "    \"\"\"\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    intersection = (preds * targets).sum()\n",
    "    return 1 - ((2.0 * intersection + smooth) / (preds.sum() + targets.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combinazione di Focal Loss, IoU Loss e Dice Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_focal=1.0, lambda_iou=1.0, lambda_dice=1.0):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.focal_loss = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "        self.lambda_focal = lambda_focal\n",
    "        self.lambda_iou = lambda_iou\n",
    "        self.lambda_dice = lambda_dice\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        focal = self.focal_loss(preds, targets)\n",
    "        iou = iou_loss(preds, targets)\n",
    "        dice = dice_loss(preds, targets)\n",
    "        return self.lambda_focal * focal + self.lambda_iou * iou + self.lambda_dice * dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINet Class\n",
    "Combina tutti i moduli e produce output finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŸ¢ SINet (Final Model)\n",
    "class SINet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SINet, self).__init__()\n",
    "        self.backbone = ResNetBackbone()\n",
    "        self.jdpm = JDPM(in_channels=2048, out_channels=256)\n",
    "        self.etb = ETB(in_channels=256)\n",
    "        self.drp = DRP(in_channels=256)\n",
    "        self.output_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5 = self.backbone(x)\n",
    "        x_fused = self.jdpm(x5)\n",
    "        x_etb = self.etb(x_fused)\n",
    "        x_drp = self.drp(x_etb, x_fused)\n",
    "        output = torch.sigmoid(self.output_conv(x_drp))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, target, threshold=0.5):\n",
    "    pred_bin = (pred >= threshold).float()\n",
    "    \n",
    "    TP = (pred_bin * target).sum().item()\n",
    "    FP = (pred_bin * (1 - target)).sum().item()\n",
    "    FN = ((1 - pred_bin) * target).sum().item()\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-7)\n",
    "    recall = TP / (TP + FN + 1e-7)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    iou = TP / (TP + FP + FN + 1e-7)\n",
    "\n",
    "    return {\"iou\": iou, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Funzione per addestrare il modello SINet con Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        model: Modello PyTorch (SINet)\n",
    "        train_loader: DataLoader per il training\n",
    "        val_loader: DataLoader per la validazione\n",
    "        optimizer: Ottimizzatore (Adam, SGD, ecc.)\n",
    "        device: 'cuda', 'mps' o 'cpu'\n",
    "        num_epochs: Numero di epoche\n",
    "\n",
    "    Returns:\n",
    "        model: Modello addestrato\n",
    "    \"\"\"\n",
    "\n",
    "    # ðŸŸ¢ Definiamo la loss function combinata (Focal + IoU + Dice)\n",
    "    criterion = CombinedLoss(lambda_focal=1.0, lambda_iou=1.0, lambda_dice=1.0)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(images)\n",
    "\n",
    "            # ðŸŸ¢ Ridimensioniamo predictions per matchare masks\n",
    "            predictions_resized = F.interpolate(predictions, size=masks.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            # ðŸŸ¢ Calcoliamo la nuova loss\n",
    "            loss = criterion(predictions_resized, masks)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # ðŸŸ¢ VALIDATION\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_iou, all_f1 = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                predictions = model(images)\n",
    "\n",
    "                predictions_resized = F.interpolate(predictions, size=masks.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "                loss = criterion(predictions_resized, masks)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # ðŸŸ¢ Calcoliamo metriche di valutazione\n",
    "                batch_metrics = compute_metrics(predictions_resized, masks)\n",
    "                all_iou.append(batch_metrics['iou'])\n",
    "                all_f1.append(batch_metrics['f1'])\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        avg_iou = sum(all_iou) / len(all_iou)\n",
    "        avg_f1 = sum(all_f1) / len(all_f1)\n",
    "\n",
    "        epoch_duration = timedelta(seconds=time.time() - start_time)\n",
    "\n",
    "        # ðŸŸ¢ Stampa i risultati per questa epoca\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, IoU: {avg_iou:.4f}, F1: {avg_f1:.4f}, Time: {epoch_duration}\")\n",
    "\n",
    "\n",
    "    # torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    print(\"ðŸŽ‰ Training completato!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "lr = 1e-4\n",
    "num_workers = 0\n",
    "val_split = 0.15\n",
    "\n",
    "full_train_dataset = CODDataset(\n",
    "    image_folder=\"COD10K-v3/Train/Image\",\n",
    "    mask_folder=\"COD10K-v3/Train/GT_Object\",\n",
    "    image_transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")   \n",
    "\n",
    "train_size = int((1 - val_split) * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = CODDataset(\n",
    "    image_folder=\"COD10K-v3/Test/Image\",\n",
    "    mask_folder=\"COD10K-v3/Test/GT_Object\",\n",
    "    image_transform=image_transform, \n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SINet().to(device)\n",
    "\n",
    "# # ðŸŸ¢ Ottimizzatore e training\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# # ðŸŸ¢ DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # ðŸŸ¢ Avvia il training\n",
    "# trained_model = train_model(model, train_loader, val_loader, optimizer, device, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"sinet_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_metrics(pred, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calcola accuracy, precision, recall, F1-score e IoU per un batch di immagini.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predizioni del modello (logits o probabilitÃ )\n",
    "        target (torch.Tensor): Maschere ground truth binarie\n",
    "        threshold (float): Soglia per binarizzare le predizioni\n",
    "\n",
    "    Returns:\n",
    "        dict: Dizionario con le metriche medie sul batch\n",
    "    \"\"\"\n",
    "\n",
    "    # ðŸŸ¢ Ridimensiona pred per matchare target\n",
    "    if pred.shape[2:] != target.shape[2:]:\n",
    "        pred = F.interpolate(pred, size=target.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    pred_bin = (pred >= threshold).float()\n",
    "\n",
    "    eps = 1e-7\n",
    "    batch_size = pred.shape[0]\n",
    "\n",
    "    acc_list, prec_list, rec_list, f1_list, iou_list = [], [], [], [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        p = pred_bin[i].view(-1)  # Flatten per batch\n",
    "        t = target[i].view(-1)  \n",
    "\n",
    "        TP = (p * t).sum().item()\n",
    "        FP = (p * (1 - t)).sum().item()\n",
    "        FN = ((1 - p) * t).sum().item()\n",
    "        TN = ((1 - p) * (1 - t)).sum().item()\n",
    "\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN + eps)\n",
    "        prec = TP / (TP + FP + eps)\n",
    "        rec = TP / (TP + FN + eps)\n",
    "        f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "        iou = TP / (TP + FP + FN + eps)\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        prec_list.append(prec)\n",
    "        rec_list.append(rec)\n",
    "        f1_list.append(f1)\n",
    "        iou_list.append(iou)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': np.mean(acc_list),\n",
    "        'precision': np.mean(prec_list),\n",
    "        'recall': np.mean(rec_list),\n",
    "        'f1': np.mean(f1_list),\n",
    "        'iou': np.mean(iou_list)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def S_measure(pred, gt):\n",
    "    \"\"\"\n",
    "    Calcola la similaritÃ  strutturale tra la mappa predetta e la ground truth.\n",
    "    \"\"\"\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    gt_np = gt.squeeze().cpu().numpy()\n",
    "    return ssim(pred_np, gt_np, data_range=1.0)\n",
    "\n",
    "def E_measure(pred, gt):\n",
    "    \"\"\"\n",
    "    Calcola la E-measure combinando precisione locale e statistiche globali.\n",
    "    \"\"\"\n",
    "    pred = pred.squeeze().cpu().numpy()\n",
    "    gt = gt.squeeze().cpu().numpy()\n",
    "    pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "    \n",
    "    # SimilaritÃ  tra pixel con pesi\n",
    "    diff = np.abs(pred - gt)\n",
    "    return 1 - np.mean(diff)\n",
    "\n",
    "def weighted_F_measure(pred, gt, beta=1):\n",
    "    \"\"\"\n",
    "    Calcola il weighted F-measure per valutare la qualitÃ  della segmentazione.\n",
    "    \"\"\"\n",
    "    pred = pred.squeeze().cpu().numpy() > 0.5\n",
    "    gt = gt.squeeze().cpu().numpy() > 0.5\n",
    "    TP = np.sum(pred * gt)\n",
    "    FP = np.sum(pred * (1 - gt))\n",
    "    FN = np.sum((1 - pred) * gt)\n",
    "    \n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    \n",
    "    F_beta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + 1e-8)\n",
    "    return F_beta\n",
    "\n",
    "def mean_absolute_error(pred, gt):\n",
    "    \"\"\"\n",
    "    Calcola l'errore medio assoluto tra la mappa predetta e la ground truth.\n",
    "    \"\"\"\n",
    "    return torch.abs(pred - gt).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Testa il modello su un dataset di test e calcola le metriche medie.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Modello addestrato\n",
    "        test_loader (DataLoader): DataLoader con il dataset di test\n",
    "        device (str): 'cuda', 'mps' o 'cpu'\n",
    "        threshold (float): Soglia per binarizzare le predizioni\n",
    "\n",
    "    Returns:\n",
    "        dict: Dizionario con le metriche medie su tutto il dataset di test\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_acc, all_prec, all_rec, all_f1, all_iou = [], [], [], [], []\n",
    "    all_smeasure, all_emeasure, all_wfm, all_mae = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            out_final = model(images)\n",
    "\n",
    "            # ðŸŸ¢ Calcola metriche per batch\n",
    "            batch_metrics = compute_batch_metrics(out_final, masks, threshold)\n",
    "            all_acc.append(batch_metrics['accuracy'])\n",
    "            all_prec.append(batch_metrics['precision'])\n",
    "            all_rec.append(batch_metrics['recall'])\n",
    "            all_f1.append(batch_metrics['f1'])\n",
    "            all_iou.append(batch_metrics['iou'])\n",
    "            \n",
    "            smeasure = S_measure(out_final, masks)\n",
    "            emeasure = E_measure(out_final, masks)\n",
    "            wfm = weighted_F_measure(out_final, masks)\n",
    "            mae = mean_absolute_error(out_final, masks)\n",
    "            \n",
    "            all_smeasure.append(smeasure)\n",
    "            all_emeasure.append(emeasure)\n",
    "            all_wfm.append(wfm)\n",
    "            all_mae.append(mae)\n",
    "\n",
    "\n",
    "    # ðŸŸ¢ Media finale delle metriche su tutto il test set\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean(all_acc),\n",
    "        'precision': np.mean(all_prec),\n",
    "        'recall': np.mean(all_rec),\n",
    "        'f1': np.mean(all_f1),\n",
    "        'iou': np.mean(all_iou)\n",
    "    }\n",
    "    \n",
    "    avg_metrics['smeasure'] = np.mean(all_smeasure)\n",
    "    avg_metrics['emeasure'] = np.mean(all_emeasure)\n",
    "    avg_metrics['wfm'] = np.mean(all_wfm)\n",
    "    avg_metrics['mae'] = np.mean(all_mae)\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŸ¢ Testa il modello\n",
    "test_metrics = test_model(model, test_loader, device, threshold=0.5)\n",
    "\n",
    "# ðŸ“Š Stampa i risultati finali\n",
    "print(\"\\nðŸŽ¯ **RISULTATI TEST FINALI:**\")\n",
    "print(f\"  Accuracy  = {test_metrics['accuracy']:.3f}\")\n",
    "print(f\"  Precision = {test_metrics['precision']:.3f}\")\n",
    "print(f\"  Recall    = {test_metrics['recall']:.3f}\")\n",
    "print(f\"  F1-score  = {test_metrics['f1']:.3f}\")\n",
    "print(f\"  IoU       = {test_metrics['iou']:.3f}\")\n",
    "\n",
    "print(f\"  S-measure = {test_metrics['smeasure']:.3f}\")\n",
    "print(f\"  E-measure = {test_metrics['emeasure']:.3f}\")\n",
    "print(f\"  WFM       = {test_metrics['wfm']:.3f}\")\n",
    "print(f\"  MAE       = {test_metrics['mae']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_predictions(model, test_loader, device, num_samples=5, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Mostra immagini casuali dal dataset di test con la loro maschera e la predizione del modello.\n",
    "\n",
    "    Args:\n",
    "        model: Modello PyTorch addestrato (SINet)\n",
    "        test_loader: DataLoader con il dataset di test\n",
    "        device: 'cuda', 'mps' o 'cpu'\n",
    "        num_samples: Numero di immagini da visualizzare\n",
    "        threshold: Soglia per binarizzare le predizioni\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Estrai batch casuale dal test_loader\n",
    "    images, masks = next(iter(test_loader))\n",
    "\n",
    "    # ðŸŸ¢ Evita di selezionare piÃ¹ immagini di quante ne siano disponibili\n",
    "    num_samples = min(num_samples, images.shape[0])  # Non supera il batch size\n",
    "\n",
    "    # Seleziona un sottoinsieme casuale di immagini dal batch\n",
    "    indices = random.sample(range(len(images)), num_samples)\n",
    "    images, masks = images[indices], masks[indices]\n",
    "\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    # ðŸŸ¢ Ridimensiona predictions per matchare masks\n",
    "    predictions_resized = F.interpolate(predictions, size=masks.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # ðŸŸ¢ Applica threshold alle predizioni per ottenere maschere binarie\n",
    "    predictions_binary = (predictions_resized >= threshold).float()\n",
    "\n",
    "    # ðŸ“Š Mostra le immagini originali, maschere e predizioni\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(10, num_samples * 3))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()  # Converti da tensor a numpy\n",
    "        mask = masks[i].squeeze().cpu().numpy()\n",
    "        pred = predictions_binary[i].squeeze().cpu().numpy()\n",
    "\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(\"Immagine originale\")\n",
    "        axes[i, 1].imshow(mask, cmap=\"gray\")\n",
    "        axes[i, 1].set_title(\"Maschera reale\")\n",
    "        axes[i, 2].imshow(pred, cmap=\"gray\")\n",
    "        axes[i, 2].set_title(\"Predizione modello\")\n",
    "\n",
    "        for ax in axes[i]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_predictions(model, test_loader, device, num_samples=10)   # MostrerÃ  10 immagini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
