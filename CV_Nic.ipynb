{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846d28c2",
   "metadata": {},
   "source": [
    "# SINet COD10K Detection\n",
    "Questo notebook implementa SINet per il rilevamento di oggetti mimetizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba989901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f24b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "tensor = torch.randn((2,3), device = torch.device('cuda:0'))\n",
    "print(tensor.device)\n",
    "device = torch.device('cuda:0')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ea9e7",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a294171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CODDataset(Dataset):\n",
    "#     def __init__(self, img_dir, mask_dir, transform=None):\n",
    "\n",
    "#         super(CODDataset, self).__init__()\n",
    "#         self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "#         self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         assert len(self.img_paths) == len(self.mask_paths), \\\n",
    "#             \"Numero di immagini e maschere non coincide!\"\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.img_paths[idx]\n",
    "#         mask_path = self.mask_paths[idx]\n",
    "\n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         mask = Image.open(mask_path).convert('L') \n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#             mask = self.transform(mask)\n",
    "#         else:\n",
    "#             image = transforms.ToTensor()(image)\n",
    "#             mask = transforms.ToTensor()(mask)\n",
    "\n",
    "#         mask = (mask > 0.5).float()\n",
    "\n",
    "#         return image, mask\n",
    "class CODDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        super(CODDataset, self).__init__()\n",
    "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.img_paths) == len(self.mask_paths), \\\n",
    "            \"Numero di immagini e maschere non coincide!\"\n",
    "\n",
    "        # Definiamo una trasformazione di base per il ridimensionamento\n",
    "        self.resize = transforms.Resize((512, 512))  # Imposta la dimensione a 512x512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Converte la maschera in scala di grigi\n",
    "\n",
    "        # Applichiamo il ridimensionamento\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        # Se ci sono altre trasformazioni, le applichiamo\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "            mask = transforms.ToTensor()(mask)\n",
    "\n",
    "        # Convertiamo la maschera in binaria\n",
    "        mask = (mask > 0.5).float()\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0144d4f",
   "metadata": {},
   "source": [
    "### Backbone feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df8c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        resnet = torchvision.models.resnet50(pretrained=pretrained)\n",
    "\n",
    "        self.stage1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu) \n",
    "        self.pool = resnet.maxpool \n",
    "        self.stage2 = resnet.layer1  \n",
    "        self.stage3 = resnet.layer2  \n",
    "        self.stage4 = resnet.layer3 \n",
    "        self.stage5 = resnet.layer4  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.stage1(x)   \n",
    "        x1p = self.pool(x1)     \n",
    "        x2 = self.stage2(x1p)  \n",
    "        x3 = self.stage3(x2)    \n",
    "        x4 = self.stage4(x3) \n",
    "        x5 = self.stage5(x4)  \n",
    "        return x1, x2, x3, x4, x5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e050d",
   "metadata": {},
   "source": [
    "### Search Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08adbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchModule(nn.Module):\n",
    "    def __init__(self, in_channels_list=[256, 512, 1024]):\n",
    "        super(SearchModule, self).__init__()\n",
    "        self.conv_list = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, 256, kernel_size=3, padding=1) \n",
    "            for in_ch in in_channels_list\n",
    "        ])\n",
    "        self.out_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x2, x3, x4):\n",
    "        \n",
    "        x2_ = self.conv_list[0](x2)            \n",
    "        x3_ = F.interpolate(self.conv_list[1](x3),\n",
    "                            size=x2_.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x4_ = F.interpolate(self.conv_list[2](x4),\n",
    "                            size=x2_.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        fused = x2_ + x3_ + x4_\n",
    "        coarse_map = self.out_conv(fused)\n",
    "        coarse_map = torch.sigmoid(coarse_map)\n",
    "        return coarse_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67098bbc",
   "metadata": {},
   "source": [
    "### Identification Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2387e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentificationModule(nn.Module):\n",
    "    def __init__(self, in_channels=2048):\n",
    "        super(IdentificationModule, self).__init__()\n",
    "        self.conv_deep = nn.Conv2d(in_channels, 256, kernel_size=3, padding=1)\n",
    "        self.refine_conv = nn.Conv2d(256+1, 256, kernel_size=3, padding=1) \n",
    "        self.out_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x5, coarse_map):\n",
    "\n",
    "        x5_ = self.conv_deep(x5)   \n",
    "        x5_up = F.interpolate(x5_, scale_factor=8, mode='bilinear', align_corners=False)\n",
    "\n",
    "        refine_input = torch.cat([x5_up, coarse_map], dim=1) \n",
    "\n",
    "        refine_feat = self.refine_conv(refine_input)         \n",
    "\n",
    "        out_map = self.out_conv(refine_feat)                  \n",
    "        out_map = torch.sigmoid(out_map)\n",
    "\n",
    "        return out_map  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3992f9",
   "metadata": {},
   "source": [
    "### SINet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6483a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SINet(nn.Module):\n",
    "    def __init__(self, backbone_pretrained=True):\n",
    "        super(SINet, self).__init__()\n",
    "        self.backbone = ResNetBackbone(pretrained=backbone_pretrained)\n",
    "   \n",
    "        self.search = SearchModule(in_channels_list=[256, 512, 1024])\n",
    "     \n",
    "        self.identify = IdentificationModule(in_channels=2048)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1, x2, x3, x4, x5 = self.backbone(x)\n",
    "\n",
    "        coarse_map = self.search(x2, x3, x4)   \n",
    "\n",
    "        refine_map = self.identify(x5, coarse_map)  \n",
    "\n",
    "        out_final = F.interpolate(refine_map, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return out_final, coarse_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33f5ca",
   "metadata": {},
   "source": [
    "### Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343bec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_metrics(pred, target, threshold=0.5):\n",
    "\n",
    "    pred_bin = (pred >= threshold).float()\n",
    "\n",
    "    eps = 1e-7\n",
    "    batch_size = pred.shape[0]\n",
    "\n",
    "    acc_list, prec_list, rec_list, f1_list, iou_list = [], [], [], [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        p = pred_bin[i].view(-1)   \n",
    "        t = target[i].view(-1)    \n",
    "\n",
    "        TP = (p * t).sum().item()\n",
    "        FP = (p * (1 - t)).sum().item()\n",
    "        FN = ((1 - p) * t).sum().item()\n",
    "        TN = ((1 - p) * (1 - t)).sum().item()\n",
    "\n",
    "\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN + eps)\n",
    "       \n",
    "        prec = TP / (TP + FP + eps)\n",
    "\n",
    "        rec = TP / (TP + FN + eps)\n",
    "\n",
    "        f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "  \n",
    "        union = TP + FP + FN\n",
    "        iou = TP / (union + eps)\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        prec_list.append(prec)\n",
    "        rec_list.append(rec)\n",
    "        f1_list.append(f1)\n",
    "        iou_list.append(iou)\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': np.mean(acc_list),\n",
    "        'precision': np.mean(prec_list),\n",
    "        'recall': np.mean(rec_list),\n",
    "        'f1': np.mean(f1_list),\n",
    "        'iou': np.mean(iou_list)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f3cde",
   "metadata": {},
   "source": [
    "### Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e93c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.0):\n",
    "\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    return 1 - ((2.0 * intersection + smooth) / (pred.sum() + target.sum() + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678f397",
   "metadata": {},
   "source": [
    "### Train Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f611c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        #print('prova', images.device, masks.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out_final, out_coarse = model(images)\n",
    "\n",
    "        loss_final = dice_loss(out_final, masks) + F.binary_cross_entropy(out_final, masks)\n",
    "\n",
    "        loss_coarse = dice_loss(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest')) \\\n",
    "                      + F.binary_cross_entropy(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest'))\n",
    "\n",
    "        loss = loss_final + 0.5 * loss_coarse\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_one_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_acc, all_prec, all_rec, all_f1, all_iou = [], [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            out_final, out_coarse = model(images)\n",
    "\n",
    "            loss_final = dice_loss(out_final, masks) + F.binary_cross_entropy(out_final, masks)\n",
    "            loss_coarse = dice_loss(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest')) \\\n",
    "                          + F.binary_cross_entropy(out_coarse, F.interpolate(masks, size=out_coarse.shape[2:], mode='nearest'))\n",
    "            loss = loss_final + 0.5 * loss_coarse\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            batch_metrics = compute_batch_metrics(out_final, masks, threshold=0.5)\n",
    "            all_acc.append(batch_metrics['accuracy'])\n",
    "            all_prec.append(batch_metrics['precision'])\n",
    "            all_rec.append(batch_metrics['recall'])\n",
    "            all_f1.append(batch_metrics['f1'])\n",
    "            all_iou.append(batch_metrics['iou'])\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean(all_acc),\n",
    "        'precision': np.mean(all_prec),\n",
    "        'recall': np.mean(all_rec),\n",
    "        'f1': np.mean(all_f1),\n",
    "        'iou': np.mean(all_iou)\n",
    "    }\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ea930",
   "metadata": {},
   "source": [
    "### Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8944f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_model(model, dataloader, device, threshold=0.5):\n",
    "#     model.eval()\n",
    "#     all_acc, all_prec, all_rec, all_f1, all_iou = [], [], [], [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         print(len(dataloader))\n",
    "#         for i, (images, masks) in enumerate(dataloader):\n",
    "#             print(f\"Processing batch {i+1}/{len(dataloader)}...\")  # DEBUG\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             out_final, out_coarse = model(images)\n",
    "#             print(\"Model inference done.\")  # DEBUG\n",
    "\n",
    "#             batch_metrics = compute_batch_metrics(out_final, masks, threshold=threshold)\n",
    "#             print(\"Metrics computed.\")  # DEBUG\n",
    "\n",
    "#             all_acc.append(batch_metrics['accuracy'])\n",
    "#             all_prec.append(batch_metrics['precision'])\n",
    "#             all_rec.append(batch_metrics['recall'])\n",
    "#             all_f1.append(batch_metrics['f1'])\n",
    "#             all_iou.append(batch_metrics['iou'])\n",
    "\n",
    "#     avg_metrics = {\n",
    "#         'accuracy': np.mean(all_acc),\n",
    "#         'precision': np.mean(all_prec),\n",
    "#         'recall': np.mean(all_rec),\n",
    "#         'f1': np.mean(all_f1),\n",
    "#         'iou': np.mean(all_iou)\n",
    "#     }\n",
    "#     return avg_metrics\n",
    "\n",
    "def test_model(model, dataloader, device, threshold=0.5, num_samples=5):\n",
    "    model.eval()\n",
    "    all_acc, all_prec, all_rec, all_f1, all_iou = [], [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(len(dataloader))\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            print(f\"Processing batch {i+1}/{len(dataloader)}...\")  # DEBUG\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            out_final, out_coarse = model(images)\n",
    "            print(\"Model inference done.\")  # DEBUG\n",
    "\n",
    "            batch_metrics = compute_batch_metrics(out_final, masks, threshold=threshold)\n",
    "            print(\"Metrics computed.\")  # DEBUG\n",
    "\n",
    "            all_acc.append(batch_metrics['accuracy'])\n",
    "            all_prec.append(batch_metrics['precision'])\n",
    "            all_rec.append(batch_metrics['recall'])\n",
    "            all_f1.append(batch_metrics['f1'])\n",
    "            all_iou.append(batch_metrics['iou'])\n",
    "\n",
    "            # Plot delle prime immagini\n",
    "            if i < num_samples:\n",
    "                plot_results(images.cpu(), masks.cpu(), out_final.cpu(), i)\n",
    "\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean(all_acc),\n",
    "        'precision': np.mean(all_prec),\n",
    "        'recall': np.mean(all_rec),\n",
    "        'f1': np.mean(all_f1),\n",
    "        'iou': np.mean(all_iou)\n",
    "    }\n",
    "    return avg_metrics\n",
    "\n",
    "def plot_results(images, masks, predictions, batch_idx):\n",
    "    \"\"\"\n",
    "    Mostra le immagini originali, le maschere reali e le previsioni del modello.\n",
    "    \"\"\"\n",
    "    batch_size = images.shape[0]\n",
    "    fig, axes = plt.subplots(batch_size, 3, figsize=(10, batch_size * 3))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = images[i].permute(1, 2, 0).numpy()  # Converti da tensor a numpy\n",
    "        mask = masks[i].squeeze().numpy()\n",
    "        pred = (predictions[i].squeeze().numpy() > 0.5).astype(np.uint8)  # Applica threshold\n",
    "\n",
    "        if batch_size == 1:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(\"Immagine originale\")\n",
    "            axes[1].imshow(mask, cmap=\"gray\")\n",
    "            axes[1].set_title(\"Maschera reale\")\n",
    "            axes[2].imshow(pred, cmap=\"gray\")\n",
    "            axes[2].set_title(\"Previsione modello\")\n",
    "        else:\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f\"Immagine {batch_idx * batch_size + i}\")\n",
    "            axes[i, 1].imshow(mask, cmap=\"gray\")\n",
    "            axes[i, 1].set_title(\"Maschera reale\")\n",
    "            axes[i, 2].imshow(pred, cmap=\"gray\")\n",
    "            axes[i, 2].set_title(\"Previsione modello\")\n",
    "\n",
    "        for ax in axes[i] if batch_size > 1 else axes:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857df341",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 6000\n",
      "Train masks: 6000\n",
      "Sample loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nicholas\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    batch_size = 16\n",
    "    num_epochs = 10\n",
    "    lr = 1e-4\n",
    "\n",
    "    train_img_dir = 'C:/Users/Nicholas/Desktop/COD10K-v3/Train/Image'\n",
    "    train_mask_dir = 'C:/Users/Nicholas/Desktop/COD10K-v3/Train/GT_Object'\n",
    "    val_img_dir = 'C:/Users/Nicholas/Desktop/COD10K-v3/Train/Image'\n",
    "    val_mask_dir = 'C:/Users/Nicholas/Desktop/COD10K-v3/Train/GT_Object'\n",
    "    test_img_dir = 'C:/Users/Nicholas/Desktop/COD10K-v3/Test/Image'\n",
    "    test_mask_dir = 'C:/Users/Nicholas/Desktop/COD10K-v3/Test/GT_Object'\n",
    "\n",
    "    print(f\"Train images: {len(os.listdir(train_img_dir))}\")\n",
    "    print(f\"Train masks: {len(os.listdir(train_mask_dir))}\")\n",
    "\n",
    "    train_dataset = CODDataset(train_img_dir, train_mask_dir, transform=None)\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample keys: {sample.keys()}\" if isinstance(sample, dict) else \"Sample loaded\")\n",
    "    val_dataset = CODDataset(val_img_dir, val_mask_dir, transform=None)\n",
    "    test_dataset = CODDataset(test_img_dir, test_mask_dir, transform=None)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = SINet(backbone_pretrained=True).to(device)\n",
    "    #print(next(model.parameters()).device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_metrics = validate_one_epoch(model, val_loader, device)  # Scompatta la tupla\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"sinet_camouflage.pth\")\n",
    "    print(\"Training completato e modello salvato.\")\n",
    "\n",
    "    test_metrics = test_model(model, test_loader, device, threshold=0.5)\n",
    "    print(\"RISULTATI TEST FINALI:\")\n",
    "    print(f\"  Accuracy = {test_metrics['accuracy']:.3f}\")\n",
    "    print(f\"  Precision = {test_metrics['precision']:.3f}\")\n",
    "    print(f\"  Recall = {test_metrics['recall']:.3f}\")\n",
    "    print(f\"  F1-score = {test_metrics['f1']:.3f}\")\n",
    "    print(f\"  IoU = {test_metrics['iou']:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c16131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
